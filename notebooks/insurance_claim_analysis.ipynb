{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Claim Prediction\n",
    "\n",
    "**Objective:** Predict the probability of a building having at least one insurance claim over the insured period based on building characteristics.\n",
    "\n",
    "**Project Workflow:**\n",
    "1. Data Cleaning & Preprocessing\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Feature Engineering and Modeling Preprocessing\n",
    "4. Model Implementation (Logistic Regression, Random Forest, XGBoost)\n",
    "5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "#for clearer plots\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Train_data.csv')\n",
    "desc = pd.read_csv('../data/Variable Description.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  print(desc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Handling irregular values, missing data, and type conversion.\n",
    "* **NumberOfWindows:** Contains \"   .\" placeholder for missing values.\n",
    "* **Missing Values:** Imputed numerical columns with Median and categorical with Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed all . in NumberOfWindows column to NaN and converted to numeric\n",
    "df['NumberOfWindows'] = df['NumberOfWindows'].astype(str).str.strip().replace('.', np.nan)\n",
    "df['NumberOfWindows'] = pd.to_numeric(df['NumberOfWindows'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checked for missing values\n",
    "missing = df.isnull()\n",
    "print(\"Missing Values Before Cleaning:\\n\", missing.sum()[missing.sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Building Dimension','Date_of_Occupancy','NumberOfWindows']:\n",
    "    df[col] = df[col].fillna(df[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Garden', 'Geo_Code']:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values After Cleaning:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Distribution of the variable 'Claim'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Claim', data=df,hue='Claim', palette='viridis')\n",
    "plt.title('Distribution of Claim')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='Building_Type', hue='Claim', data=df, palette='viridis')\n",
    "plt.title('Claims by Building Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Claim', y='NumberOfWindows', hue='Claim', data=df, palette='viridis')\n",
    "plt.title('Number of Windows vs Claim Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Modeling\n",
    "* **Categorical Encoding:** Converting text labels (V, N,...) into numerical format.\n",
    "* **Feature Selection:** Dropping ID columns `Customer Id`.\n",
    "* **Scaling** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.copy()\n",
    "\n",
    "#Dropping irrelevant columns\n",
    "df_model = df_model.drop('Customer Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encoding categorical variables\n",
    "cat_cols = ['Garden', 'Building_Fenced', 'Building_Painted','Geo_Code','Settlement','Building_Type']\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define  features and target variable\n",
    "X = df_model.drop('Claim', axis=1)\n",
    "y = df_model['Claim']\n",
    "\n",
    "#Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "* **Logistic Regression:** A simple linear baseline.\n",
    "* **Random Forest:** An ensemble method robust to overfitting.\n",
    "* **XGBoost:** A gradient boosting algorithm optimized for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# scale_pos_weight accounts for class imbalance (approx ratio of 0s to 1s)\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "xgb = XGBClassifier(scale_pos_weight=ratio, random_state=42, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Comparing models using F1 Score (due to class imbalance) and Confusion Matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    print(name)\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_pred):.4f}\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Logistic Regression\", y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Random Forest\", y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"XGBoost\", y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary and Insights\n",
    "\n",
    "* Model Performance: After experimenting with Logistic Regression, Random Forest, and XGBoost, the Logistic Regression model emerged as the most effective tool for this specific dataset. Its high F1 score indicates it successfully balances precision (avoiding false alarms) with recall (catching actual claims), which is crucial for risk management.\n",
    "\n",
    "* Key Risk Drivers: The analysis revealed that Building Dimension and Geo Code are significant predictors of insurance claims. Larger buildings in specific geographical zones show a historically higher probability of filing claims.\n",
    "\n",
    "* Bias Mitigation: The initial data showed a heavy imbalance (fewer claims than non claims). By implementing class weighting in our models, we successfully forced the algorithm to pay attention to the minority class, ensuring we don't miss potential high risk policies.\n",
    "\n",
    "* Recommendation: We recommend deploying the Logistic Regression model as a Pre Screening Tool for underwriters. This allows for automated risk scoring, enabling human experts to focus only on high probability cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model for the App\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(lr, '../models/claim_predictor.pkl')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"\u2705 Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}